Hello everyone. Good evening. Am I audible? Hello. Am I audible to you all? Yes. Thank you for the confirmation. uh Goro right like I have shared a small uh recorded video in the group. Did you watch it? So that will help you to fix the notebook issue. Give me a minute.
I'm so sorry. It's an official call. I need to take it up.
Um, okay. So regarding the C credentials there is a clarity uh let's wait others to join so that I can uh convey that once. So before then I just want to understand uh do everyone got your prologs issue just raise your hand over here.
Yes, happy to hear all all are having the prolog. So did you all go through the materials which I have posted in the prologue? Understood Adida. Uh but I cannot just ignore the students queries.
Right. Okay. If you are still facing the issue um can you please mention what is okay I will share a uh form by end of the session so where you can fill it up okay so regarding the IBM C access how many did you all receive any mails uh regarding the credentials Okay. So those who are received the credentials uh please listen please listen um let me share my screen. Give me a minute. So everything has been mentioned in the mail very clearly.
Okay. So you got the ID and password, right? So uh just click on the link. It will redirect to this page. In this page, you have the option of sign in.
Yeah. Just click on the sign in and they directly you can put your ID and password. Just click on login. You can able to get into the interface where you can see your course. Okay. Whereas those who are not received the mail, no problem. I will share a link in chat.
Okay, you just click on the link. The course has been assigned to you all. Yeah, the link is there in chat.
Please do click on the link.
See this link is for those who not received the email the credentials email please do click on the link. Yeah. Once you click on the link, Once you click on the link, did you all click on the link?
Shall I move on to the next? So here you do have the option called register. So please click on register. So here you have to give the full name. Please understand the name which has to be print on your certificate. Do check the spell error.
uh give the correct name over here and in emails give the registered email id the email id uh which you are using for the IBM C uh IBM cloud access okay and the public profile name is optional you can give any name whatever name you are just giving it will be added in your profile okay and you have to generate the password once you all done just click on create an account for Those who not receive the um credentials, please click on that.
Okay, I'm sharing the link today again in the chat. See um many of you like those who are not received the email from Android prolarn you can use this link with this link you can able to register yourself. Email is your uh your academic official email. the email which you are used for the course registration. Those who received the credentials in email there already they have given you a link. Just click on the link and use the credentials and sign up where you can uh see your course. Okay.
Those who not receive the credential please click on the link register yourself then login. Okay. If you notice this is a dedicated port for VIT.
Are you able to see the um course Akash Akash Kumar A. So see this is not about the Yeah.
[Music] Uh of course. Yes. Ra. If you received the email credential then you skip this part. This is not for you. Once you all done, once you all able to see the page, kindly update me.
The the site is working very fine. Yes, Deep Pikashri. This is for the students who didn't receive the credential via email.
In email, you will be receiving a ID and password. Those who received the credentials, don't do the skip step.
Skip it up. Those who not received the um email credentials email, please complete your registration and then login. Don't register your course with your personal email id. You should not use that. If you use your personal email ID, then you cannot able to see the course and you cannot able to see the certificate. Um if you register the course with your personal email id then we can use it up. So what is the email id which you used for the payment which you used for the course registration use that email id. Yes. After showing once you sign up it will be uh showing that view geni WhatsApp a course like it will be like this. I will share my screen.
1 minute. I'm just sharing it again.
So you will be having different uh so in courses you could see this course genai using what's next. So if you click that course where you can see your course module am I clear to you all?
Okay, please do check your uh internet. So, did you all able to see the course? Site is working. See I use the same link and I have loaded in my screen. It is working very fine. A if your name is written wrong then uh drop a email.
Okay. Okay. Understood. I'm repeating again. See those who have received the please listen. Uh don't drop any any queries in chat as of now. Please listen first then you drop all the text. Okay. see those who have received the um email. Okay. So today you will be receiving a email from Android Technologies uh regarding the IBM Credentials. Okay. Those who have received the credential email in that email there is a link please do click on the link and there they have mentioned the user ID and password. So use the those credential and uh sign in to your account.
Okay. And those who not received the credential, I'm sharing a link in chat. One second. I'm I'm sharing the link. So link is there in chat. Okay. Uh you have to click on this link. Once you click on this link, it Okay. Even I'm getting the Okay. Maybe I just open incognito.
All right. Actually the service is temporarily un I'm also getting the same error. So maybe let's uh try after some time.
Uh I I just send the link only for those who joined recently. Okay. So I think I think the site is down. Uh we try after some time maybe at end of the session maybe after 7 uh p.m. we can uh try it together. Okay. Then we can check it up. Okay. Um now let's focus on today's session and those who uh those who are able to log your credentials successfully and if you are facing any uh name er spell error or anything wrong in your site take a screenshot and just mail to us. Okay. We will fix it up. Okay. Um Aditya did you join recently? Just now I told you those who not received the credential follow this method click on the link and there you have to register and then sign up. So during registration you have to give your uh uh registrated email id that the email id which you have used for the course registration that email has to be given over there and the name has to be correct. However you want to be printed in the certificate the same way you have to mention in the name field.
My humble request, don't spam the inbox with the same text. See, I told you those who received the credential use the credential and the link which is mentioned in the mail.
Again, you're asking uh is this a site?
I received this in email. Should I have to use this or use that? Please listen carefully and follow the instruction.
Not a problem.
Thanks to thanks for teaching me Adita.
Okay. Okay. So if the course progress is showing zero which means you have to pro see if you go to the module one. Okay.
And if you uh look into the content your course progress will be improving. So it it it will be showing the how much you have go through in the particular course. So that is your progress. Okay. In every module you will be having a skill check. So you have to complete the skill check. So totally we do have six modules. All the six modules you have to complete with a skill check.
Okay. At the end you will be having the final assessment. I will uh and uh I will teach I will let you know how to climb your certificate as well. Okay.
All right then. So what do we discuss in the last class? In last class what do we discuss So do everyone is having the IBM uh cloud account access which means what's an X what's an AI access right so there is no issue with the tool right everyone is having the IBM cloud out. He Yes, last class we have uh learned about transformer and how to use transformer. Okay. So listen, stop posting your queries regarding the account creation um CB C portal uh regarding the um all the portal issues.
Please don't post anything in the chat as of now. We will look into it after 7 p.m. Okay. Now, let's look on to the today's topic.
Okay. So I guess 90%age of you all able to access your IBM cloud IBM what's next. Okay let's continue on the task which we done on yesterday. So I'm just getting into my IBM cloud login and I'm sharing that screen.
Please log on to your WhatsApp yesterday's Four.
Sharma still you have not complete the uh registration itself. Go through the document which we have shared. There we have mentioned all the steps with clean sword. Okay. So once you land on this dashboard over here we have to click on the hamburger icon. go to the last where you can find your WhatsApp next. So I'm just going to launch my WhatsApp AI. I'm not going to create a new project indeed. I'm just going to use the yesterday's project. So how to do that? So if you scroll down over here, you can jump into the lastly used projects. So I'm just selecting this T1.
This was my last uh task class used project.
So last class we have just installed the uh we just learned how the transformer works what how how it is tokenization.
Yeah. So why this tokenization is important? Why it is matters?
Why it is important? Can anyone tell me? Because before NLP model uh like uh before GPT2 or BET the text it need to be convert into if you want to convert into number we have to taken the token ID and from the token ID the it we have to convert it to the input to the model.
Right?
So uh and uh if you want to decode the predictors again the token back to the readable text. So this tokenization uh it just bridges the this gap. Okay. Now we're going to uh in last class we learned how uh transformers work. Right.
In today's class we just going to understand how it predicts the text. Okay. So we're going to use a pre-tained GPT2 model. With that model, we're going to understand how it is predicting the uh next word. Maybe I can use this PPT to teach you first.
Um, so did you all open your uh noted. Uh so maybe we can have the doubt clarification session after some other day when you are free moxita I think in today's uh in today uh in WhatsApp group I have shared the uh document not a document a small uh recorded clip in that how to fix the assert issue. So please check on that. Uh just a minute I will share the link.
Okay, I have shared the group link in chat. Okay, so there you have to choose this lot one group.
That's okay. Fine. No need to be sorry. I can understand. So you people don't you're not there in any group.
The reason why I have shared that uh clip in the group means see I have received so many mails. Okay. When I want to check your um interface I need the verification code. When I approach few mails like when I contact their verification codes I have not I have not received it for a long time. Obviously it's a waste of time for you and us right so that is the reason I have just recorded the small clip and I have posted in the group I thought it will be helpful to you all Okay. Shall we uh is my PP is visible to you all? Which screen are you seeing now? Okay. So, once you all loaded the uh last class code.
Okay. Okay. It's the uh maybe uh I will share the link little later. I I need to get the link. Once I received the link, I will share it with you.
Okay. One sec. I'm sharing the screen again. Okay. So now we're going to understand um how um the GPT2 is predicting okay how it is predicting the next text using the pre-trained GPT2 model. So first why we have to do this why we have to predict the possibilities with GPT2. One sec. I have multiple screens opened. Okay. So, when you use a language model like GPT2, you're asking it to predict the next word or a token in a sentence, right? When you're doing this, the model does not just blur one word. Okay? actually assign to predict uh assign uh assign to a probability to every word in its vocabulary and picks the one which has the highest like code.
Uh remember last class we have seen right uh it will take the similar word in the similar word it will match us with the vocabulary from the v vocabulary it will be taking the vectors value right so when you're doing that it will not just do for the one word it will do for the all the most u likehood words okay so why why this is useful how this is useful okay it will uh help us to understand how the model thinks yeah by looking at the probability you and see how confident is the model to predict the next [Music] word. Okay. Uh so so that you can understand it model confidence and the second uh importance is like you can understand the text generation. So you can give a sample or a choice. The next word based on this will be their probabilities. Okay. And when uh you can take a deep understanding study which token will consider and why it is considered so that you can analyze the model.
One sec. One sec. Okay.
Connected. Now you can debug the model behavior. For example, if the model makes any bad uh prediction, okay, it took long time to predict. Yeah, you can understand why it is taking long time.
You can debug the code. you can understand the reason behind the uh delay. So these are the reasons or these are the main uh in in this way this predict probability in with GPD is helpful. So are you clear like why we are going to do the predict probability with GPD too. Shall we move on to the next? Shall we see the code how it is working and then we'll come back to the theory again? Can I get any response? My screen is not visible.
Thank you Navin. Okay, now let's see how we going to predict the um possibilities. So I'm just stop sharing the screen. I'm sharing back the um my what's the next AI. I'm going to the last and here I'm just going to write my code first code. So first what I have to do I have to load all the pre-trained models like I have to load the uh libraries class which class I'm going to use it up. So first I'm going to import a transform. Maybe I just go here. Yes I'm going to import.
Sorry already I have imported the transformer in last class. So no need of it because this morning only I just relaunched my uh this notebook. So if you not done please install this pip install transformers. Once you install the packages then you start coding it.
Okay. First please install the pip install transformer. Once you've done that then you call the inst in in uh transformer. So from transformer transformers I'm going to import the class auto model for casual element.
So this is this transformer is a popular Python library which was which was following the uh hugging uh hugging phase. Okay. This this hugging phase library will provide you the pre-trained model for NLP task. Okay. And here we have used the um auto model casual LM.
This is automatically loads most suitable casual languages for predicting the next word in a sequence. Okay. After this, I'm just going to download the GPT2 model in my local and then I'm going to predict the uh tokener tokens. I'm sorry. Where am I?
So over here I'm going to create a variable called GPT2 equals with help of automodel for it follows camelin case.
So please follow that casual. Okay. Over here I made a mistake. Casual LM dot from underscore I'm just going to train the pre-trained GPT 2.
Yes. After then I'm just going to pad the token underscore id equals. So every model has its own tokenizer, right? I'm just calling the tokenizer dot e oscore token id. So after then we can print it up the output. So I'm just giving output equals GPT 2. Already we have used this IDE in the last class transformer. I'm just calling that and after then I'm just going to use out [Music] Sorry.
Outputs dot logits dot shape shape. Okay.
So you all done done completing the code. So here first we have imported the uh library after then from the pre-trained GPT2. This will download and load all the pre-trained GPT model. Okay uh model and configuration to your local local environment. After that we have the uh pad token ID equals to tokenizer EOS token id. What this will do? This will set the pad pad uh padding token to the same ID as at the end of the sequence token. Okay. So this is often don't doesn't because GP2 does not have the padding token by default. So we have to include it. So if I run this code, what will be happen? it will be download the um uh configuration file and the save tensor file to the model. So let's check the output first. Okay, it is not closed properly.
Uh, where I made a mistake. Actually, I made a mistake here.
Okay. Um, see uh it's it is again showing no module name transform which means my code was not able to recognize what this transformer is again. So again I have to install pip install transformer. So once it is installed, I'm just taking the code and I'm pasting here and I'm running it. I have not used the output as of now. I'm just give Okay, I missed to give that.
Okay. So it will give the torch size.
See over here if you notice we have used something called logit. So what this logit do any do you have any idea about it what this logit? Have you heard this term?
I have not seen the chat so far.
Which everything in the sense the code which I'm showing over here Okay.
Um, good question. The thing is I'm just showing you the example like explanation how this machine is working up. You should know the concept. You don't need to memorize. So once you start writing the code for the small small application like this, it will comes in practice. So do you if I give a task of writing a uh simple sim uh login simulation would you like to memorize remember the code? No right same happens. It just happens because of the practice. Same will happen over here also. Just understand how it is progressing. Oh, you want me to share the code? Sure. I will be sharing it over here.
So, just copy the code.
See when we going to use um foundation model it will beame very easy.
Okay. So now let me explain what is this logits. So this logits it's a raw score.
Okay. What this will do from any model okay it carries the raw score. Okay.
Before applying uh softmax uh logics to converts to a probability it will carries a raw code. Okay. So this GPT2 outputs a log for every token in its vocabulary. Okay. For each input position. For example, so if you are having seven tokens. Okay. Then the GPT will gives seven sets of predictions one per token.
Okay. And each and for each uh you will be having uh some possible next token.
So that possible next token also will have the login. Am I connecting So if you are clear with this, shall we move on to the next Shall we move on to the next So that is why here you got the output the torch size. What is the torch? So torch size is 1 comma 7 comma 50252. What it means? Here we have given the input which has seven seven tokens which means seven sets of predictors and one per token and for each we will be having 50 to 275 possible next token with logits.
See why we have used shape here? What is the purpose of using shape?
So when you use numpy like when you use array okay so if you want to know the array size so we will be using the shape right so whether it's a 1D matrix 2D matrix the size of the to know the size of the array we will be using Which code you are sharing in chat? Is this a last class code? I think okay maybe uh if tokenizer is not defined which means your library has not defined properly.
Maybe uh like previously I have used some other code for practice.
Um maybe uh at end of the class I try to share all the code. Okay. While uploading the content I will share the separate um Python file. I I just downloaded a Python file and I will share it with you all. Okay.
So if you are clear with this topic shall we move on to the next topic. Do anyone having a doubt? Okay. Now what we going to do with this model? We're going to predict how likely each token in the variability is for uh like positioning each position in the input sequence. Okay. Now what we going to do? We're going to predict the next token prediction using GPD2. So shall we try the next code?
Do anyone have uh any doubt or glitch in this code? Okay.
Uh please do check the spellus.
Okay. Now let's see how to do the next token prediction. For that first I'm going to give create a uh uh create a variable final logits. Okay. So over here I'm just calling the GPT to so in this id is only I have my input dot logs I'm passing the array 0 1.
Okay, then just a second. So over here I'm just creating calling the logits equals sorry dot argument max.
I'm just calling this method. Okay, this is the next set of code. So over here what we are doing first we are creating a variable called log final login. Okay, in that we are using GPT to ids. What this? This is the input token. Yeah. Uh in this uh we have the input token in the ids. Okay. Dot log. This will gives you your tensar shape. Okay.
Um actually here I have to give a negative indexing by mistake I have just g positive. So from the last has to calculate. So if one is the batch size okay then what is the meaning of minus one which will select the logit from the last in input token. Right.
So here we have given zero. Zero will select the first batch. Yeah. Which means first uh set first batch size.
Okay. Whereas uh minus one that will select the last batch size. So which means GPT2 will predicts the next token after the current input.
So now the variable final uh logits will hold the raw score for every possible token that GPD2 thinks could come next. So how it will compare? It will compare the uh most likeliest vocabulary uh sorry um words from the vocabulary. Okay. And in next we are just creating uh in in the same uh with the help of the final logit we are just calling argument max. This will find the highest uh index value. Now if we run the code. So this is my highest index value which is 1755. So which means what it is predicting this GP2 top choice for the next uh token ID is 1755. Now let's decode this token. Let's understand what is the word for this particular token.
Um maybe I can share this code in chat. Are you able to follow me? Do anyone having any glitch or anything is not clear? Kindly let me know.
because that will store the raw um score of each word. Okay. Then then I will show you uh quickly how it will do the next prediction. Maybe I can take this Okay. Now let's take we have got the token. Now we need to understand what is the value which is there in the token.
Right.
[Music] So here I have just given the tokenizer decode. So this will convert the predicted code token ID back to the human readable text. So if I run this code I will be getting the value which is night. Okay. Uh now with this we can able to um we can able to create the top 10 tokens. I will be giving the code Okay. So, it will predict the top 10 uh next predictable words with our input string.
So over here we have used the uh torch torch pyarchch library. Uh it is used for tensor operations uh deep learning neural network. Okay. And here we have with the help of top k. So torch do top k. What this will do? This will call the tensor comma k which means it return the top k value. And this and this is from the tensor. Okay.
Maybe I can share the code in chat as well. You can all run it up. It just predicting the uh with the give with the given input last word, it will predict the next upcoming word.
I think your uh um our last word is strawmy. Of course. Thank you.
So in a looping uh I just uh printed the 10 index indices value of next predictions.
Okay, are you clear with this code? Now let's see how GBD uh predicts the generating the next text. So if I give one input text from that it will be printing uh printing the next generative text. For example, if I give uh hello. So with that it will automatically generate the next word hello world. So here I'm not going to create from the scratch.
Already I have the existing code. From the existing code I'm just going to use only the conditional statement. And with that I'm just going to print it up the statement.
Okay. So already I have the code. I just copy paste this code and I will explain the code to you all.
Okay. Okay.
So just to I I I was just given the it was a dark and stormy night. So after then it it will be giving the prediction predicted words.
Okay, sorry generative words. Now let me decode this um code. So over here we have used the if condition if tokenizer do.pad token is null. So what it will be doing it will be checking if GP2 is does not have defined any pad token. So if pad token is this this padding is used when input sequence need to be the same length. So during that we have to use this padding.
Okay. So if we set the PA token to the EOS token which means end EOS means end of state sentence. Okay. Uh so we are just setting P token to the EOS token so that we can avoid errors. So that is the reason here we are just um assigning setting the values. So if the PA token is none. If there is no value then in the pad token variable we are just storing the uh tokenizer EOS token.
So now both will carry the same length, same word. Okay. So after then we have just give output id is equal to gpt2 dot generative. So what this uh generative will do? GP.generative it tells the model take the input continues it by generating up to 20 more token. So that is why id is your tokenized input and new uh max new token equal 20 which means generate 20 new tokens and there we have the padded token id equals to this will helps the model properly formatted the output internally. So over here I'm just creating a variable output ids equals to and there I'm just using GP2 generate. It will tells my model to generate top 20 more tokens from the input ids. Is it clear?
And after then I just decoded the output with a variable called decoded text equals to tokenizer dot decode. And inside that I we have just given output id is equals uh like array uh index value of zero which means I'm converting the list of token back to the human readable text. So I have t given the input token and output token. So why I'm just giving that input and output? I just want to check whether the token of our given sentence, the original sentence and the generated sentence are same or not. And finally I just generated the printed uh generated text the text readable. So any doubts in the code? Can I share the code in chat?
This word comes from GPT2 which is already pre-trained model NLP model. Yes, always it will pick the highest rated logit. Yes, let me explain it once again. Uh you want me to explain on the token ID third line?
Yes, this one right.
Okay, I hope you understood this. Output ID is equal to 20 till 20. You got it right. Till this you got it right. So I just created a variable output ids. Okay, in this variable using GP2 dot generator, this will tells the model GPT2 to generate maximum new 20 tokens from this input token. Okay, till this it till then it is clear, right? Okay. After then we have the pad token ID equals to tokenizer dotpad token ID. This will helps the model properly format the output internally. Am I connecting non what is your output?
ips and uh if can you please share your code and what was the your input string?
uh okay so you will your output will be generated based on this your input is uh input sorry nights in the London but my input is it was a stormy night so depend upon your input statement the output will varies Okay. And rishik like what if we increase the number of new tokens then the sentence start repeating itself. Uh why this is happening? Good. See when you increase the number of token which means max new token you might see the repetitions why this is happening because GPT generate one text at a time right so so it's always based on the previous token so if you keep on repeatating the sentence so from the previous token it will again generate the Okay.
The logic is like from the token it will be taking the vector value and it will be checking with the most uh likely nearest value in the vocabulary from that it will predicting it. That was the logic behind this. Okay. So if you clear with this uh shall we see about LLM and foundation model? Shall I stop share my screen? Should I show the PPT or do you have any other doubts?
So I'm sharing the like someone is asking the for the video I'm sharing it again.
So do anyone have doubt with the code which we have discussed? Shall we start with the next topic?
No, you don't need to memorize all this code. Once you're used to it, you can uh write your own code.
Okay. So, someone has what's WhatsApp me with a uh issue. Now, should I have to take the session or fix the issue? I told you very clearly. Please don't uh share your um issues in WhatsApp during the session time. It will interrupt.
Okay? Maybe after the session you can share it up.
Thanks for the understanding.
Okay, I'm stop sharing my screen and I'm sharing the PPD with you all once again since you're new to it, right?
satisfy you. You felt this very long uh very hard to rem like should I have to remember like like like that questions are arising for you all. See once you are get well versed with this this will make a piece of cake for you all. So this was a predict probability uh code which we have discussed and this was the output which we got and this was the next predictor code. Oh, I forgot to add the output for this next pred. Okay, here we have the output next token prediction. And here we have the top 10 prediction. And this was the generative text prediction. Okay. Now we're going to understand how transformer work. Do you uh any idea about it? Like we were doing this for last two days, right? How we are working with transformer. So how this transformer works?
What is happening from the scratch? What was the thing before encoder decoder push?
So what is happening?
Okay, maybe I will show you the PPT. You try to guess it up. The first step is input embedding. Could you able to recolct the things which we have discussed in the previous classes? First uh I if you want me to uh put it in a simple term we can say this input embedding as input representation. Over here you we will be giving a input to the model. The model will break down the statement into small small tokens. The token can be a word subword or a character. So literally it is converting token to a vector. Okay. Then each token is converted into a vector by using the embedding layer. Why this is happening?
Just because machine cannot understand the human speaking languages. It's know only zeros and ones. So the text converted to the vectors. So this was our first step right. Very good.
So after this we do have the next steps a after converting it we do have the positional encoding. Yeah.
So over here the transformers do not recurrence which means over here the transformer uh like in RNN it can it can be uh repeatly do the same thing but over here they cannot do like that. So we are adding information to each token so that the model will know the order of the words. So you all understand the positional encoding.
See after um so now we have the token. The tokens are generated. For each token we are adding the positions the uh the arrangements of the each word. then it will be given to the input to the model. Okay. And the third one is multiit self attention mechanism. So over here each token looks all other token in the input to understand the concept. For example, if we have the cat sat on the mat, the word cat or sat uh looks attention of the next other word.
If it is cat, then it will look at the mat or sat. So to understand this meaning better. So this will be used by the uh quitiki value.
Clear. Okay. The next step is connections and normalization.
So over here what we will be doing we will be running the multiple capture types multiple words different uh types of rel words and we'll be finding the relationship between each words. After finding that we will be looking for the attention. Okay. So uh now we have the original input text. Yeah. And after then we have to normalize to keep the value in check. Why we are doing this?
Before that we do have the uh feed forward network FFN. So what there what we will be doing? We will be keeping each token passes through a small neural network. So small it is same for all token.
Once it is passes to the uh small neural network, we will be again getting back all the uh um input uh uh tokens, original tokens back to the [Music] output and we have the stack layer. So over here we will be repeating the repeated the uh above steps again and [Music] again. So in every every layer repetition we will be understanding the uh refines of the input and at final level we will be doing encoding and decoder.
Then after then we will be doing the training. So in training we will be using the supervised learning because we know the lab we we are going to use only the labeled inputs. After the training we do have the inter inference. So once the model is when the model is ready now you can use to import the model you can use to uh do it with the um unlabelled data. So this is how your transforms are working. Okay.
Hope it is clear. Now we're going to understand what is LLM. What is LLM? Okay. So before then we can summarize uh first we have input uh embedded input. Okay. Over here we are just finding the sematic meaning and then we have the positional encoding.
Over there we are just doing token order. We are arranging the tokens again back in an order. Then we have multi head attention. Over here we are just learning the relationship between the each token forward. And the next we have the uh residual or layer norm. Over here we are just doing a stable training.
After then we have the for feed forward.
There we do deep transformation. Then we have stacking where we are doing hierarchical featuring. Then output which is predict predictions. Uh the model started to predict. Then we have training and inference. Trading is learning from the examples and inferences where you are applying new data. So in uh this is a flow how the model learns.
Now we're going to understand LLM large language model. So what is the use of this LLM?
So with the help of LLM we can able to uh train the model to generate human languages right and we have before understanding the uh foundation model can anyone tell me how this LLM [Music] works the same As over here also it will take the input and the input will be uh like it will tokenize the input text convert to embedding then it will apply the transformer block the one which we have discussed before and then it started to predict the next word or next token and decode the output. The token will be decoded to the human speaking language.
Give me an example for LLM. Chatbot. Very good.
GPT. Chat GPT. Yes. Chat GPT version three and version four.
It lama Alexa.
Good. Google Bird, right? Good.
So in foundation model it's a large uh with the help uh in foundation with the help of foundation model here we will be having the pre-trained models. So with with with this we can able to create the um generative textes. So this is how it will be working. So we do have llm I'm sorry. Okay. So here in LLM it like greater text, right? So whereas in foundation model uh it works on unlabelled data. So the foundation model is a larger uh it's trained on unlabelled data where we no need any tags. Okay. It is uses uh this foundation model uses the transformer architecture which means the engine behind this is chat GPT and bird etc. Okay. The foundation model is a base for many applications like if you create any application on vision uh audio text yeah and this will be the uh base. Okay. So this is like uh one tool uh that can be fine- tuned many task that is foundation model. So it can fine-tune chatting, it can fine-tune analyzing image, it can fine-tune summarize writing summariz summaries and it can fine-tune generating music. So LLM it's great only with text.
Okay. When it comes to foundation model, the engine that has all the power, which means uh it it works based on transformers, it can do uh on multiple things like text, image, audio, video and uh then we do have genai. So genai is a real world tools that can create content for business and life.
Okay. Next we going to understand about H. So here we have different categories of foundation model. Yeah, we do have IBM granite model, open-source LLM, code generation model, multilingual model, instruction model. It it has its own um thingy. Okay. So, what this IBM graded model will do? So this model okay uh it uses the Watson uh we will be using Watson AI environment okay where we have many uh models like granite 13b uh grain we have different versions in granite okay so these these model help this if it is a granite model which has helped us to create uh chat bots summarization code generation Yeah. And if it is a open-source LLM, this is for general purpose check text generation. Okay. And uh in category in in code generation model category uh we will be using pi see if you give a text it will generate in a um programming languages. So it it can you can use python, java, sql automation script.
Okay. So those all comes under the code generation model category. So if you want to use code generation model category then we have to use the model.
Model name is code lama uh instruct granite uh 34b code instruct. So these are the model that you have to use if you want to create application which could which could generate the programming languages. Okay. So if you want to create an application which could generate the uh non-English application like if you want to create some um global uh some like human speaking languages like Japanese, Arabic, malalam. Yeah. So in that case you have to use the multilingual model. In that it comes under the multilingual model.
So there we do have different models like you you have to use granite 20B um and uh JS 13B chat. Okay. So uh in general we have all these category in each category we do have different models depend upon the requirement we have to choose the model and we have to work on it. Am I [Music] connecting? Is anyone there in the class? Yes. Thank you for the responses.
Yeah. Okay. So, uh do everyone clear with the categories?
Okay. Now let me ask you one question.
Okay. If I'm going to generate an application, in that application, my users can generate if they give um uh if they give a prompt, they can uh generate SQL queries and then which comes under which category? It comes under which category? I'm repeating the question.
I'm going to create an application and that application is going to generate the SQL queries automated SQL queries.
Okay. Then which it comes under which category? Good.
Yummy shahiti sagnik.
So which means only three person has listened to the class. Oh my god.
Hangita raidi goautend.
Good. Okay. All done. All done. Okay. I'm going to create an application. Uh that application will llm no adita adita naran I'm sorry narin not llm. Yes, the answer is code generation category. Okay, I'm going to create an application. So that application will be um if you give a text it will generate the uh Spanish text then which it comes under which model Huh? Very good. Good.
Chirashita hashit Ashish. Good good good everyone.
Okay. Now I'm going to create a chatbot.
It comes under which model?
LLM no see look at the category look at the category no it's not instruction it's not geni which model good roshini As of now only one person has given the right answer. Huh? Anul good pretty good.
No, it's IBM granite model. Yeah, in this category we do have the models like granite 13b, chat v2, granite 20b, code inst. So this help us to create a chat bots summarization, code generation.
Okay. Now we're going to understand how to choose a right foundation model. On what basis we have to choose the model So we do have different models and its purpose. So uh since you got the IBM C access you can go through that.
So here they have given few uh language few models and uh they have given the insight which are good and good at which which uh thing. Okay. So for example look at the image. See we have granite 13b chatbot v2. This model is good in classifying extract generate Q&A and summarize. But this model cannot create code generate code.
Okay, if you look into this um version granite 13b uh inst v2 this is also good in uh classify extract generate Q&A and summarize. So in your curriculum they have given many models with that uh uh their specialty they are good in at which particular part. Okay. So depend upon its um popularity we have to choose the model.
For example uh here uh let me give an example. I want to create an application which is good in translating. So you will be choosing which model from the given context from the given context. You will be choosing which model.
Please drop your answer.
Okay. Many said you will be choosing granite 20B multi can be used in step model for chat.
Okay. All right. Now, let me give you a what was the question that I asked?
Okay. I said if I'm going to create a chatbot which model will be choose from the given table. Okay. 20 because it has many functionalities. Okay.
any of the first five. What are the first five that I have? I think uh I if if you ask me I will choose the first one. I will choose granite 13b chat v2. So name the name itself mentioned in that this model is specialized in chat. So that we can able to create the chat bot.
Yeah. But if you notice here, we do have a dedicated model which is meant for Japanese language. So, granite 8b Japanese. If I'm going to create application uh where uh to create a Japanese-based uh bot or a Japanese-based application then I will be using this model only because this is very specified and if you notice over here we have for the code generation. So here we have 3B, 8B, 20B. So each model has specialized in any program different programming language. For example, 20B is specialized in Python. If I'm going to create application which can generate Python code. Okay. So then I will be using this model. Is it clear? Okay. Now uh I have few general info announcements to make. So tomorrow you will be uh as of now with this we are completed first three modules from your uh given uh syllabus. Okay. So tomorrow you will be having your quest assessment on prolon. No worry before then prolon will be uh prolong issues will be fixed for any everyone.
Okay. So get ready for the assessment and that uh and that evaluation carried out for 20 marks. I'm sorry 15 10 marks.
So can anyone summarize what we have discussed today?
And if you have any doubts, please drop your doubts in chat and I will be sharing you one uh Google form. So please fill the Google form.
Um so it will have the all your uh it is just for the portal access and if any portal if you don't if you have any issue with the portals please let me know.
Meanwhile, please drop your feedbacks about the session like are the session up to your expectation? Uh do you want to add anything to the session?
Please drop your um feedbacks.
Give me one minute. Almost done. I will be sharing the uh Google sheet with you all.
Okay, the link is there in chat.
uh it is a quiz only so tomorrow assessment it's a quiz I have shared the uh Google form link during the session will be it will be happen first three modules like whatever we have covered so far the basics of geni Uh please uh fill the Google form. Ready for 10 marks.
So please fill the form. Okay. Regarding the feedback, I have received um two feedbacks. Thank you so much Adita.
And I saw one more feedback but I missed it. I'm sorry. And thank you for the other person who shared the feedback. Yes, by end of the day I will be uploading the module 3 PPT in LMS. Sure, I'm sharing the uh Google form link once again. Uh please use it. And in uh prolong access we do have forum. Okay.
So in that forum you can if you have any queries uh instead of mailing WhatsAppapping you can even you can use the forum. So there we can discuss everything it will be on channel where you can see what are the queries you have asked so far.
So quiz timings tomorrow in session will be the quiz will be happen and questions will be from basics of gen AI for 10 marks.
Okay, with this like uh if you if you don't have any issues then thank you so much for joining and thank you for listening. We'll meet tomorrow. Bye.
Take care. Good night. So please fill the Google form if you have any issue. I will escalate to the technical team.
Aki did you fill the uh Google form? In the Google form you have to mention if your uh IBM C access skill network is wrong. There you have to fill it up.
Then only I can escalate to the technical team. That is the reason I'm sharing the uh Google form as well. So I I I already mentioned quiz timing will be quiz will be conducted during the session. This is assessment. Apart from that we don't have any assessment tomorrow. Okay. I'm sharing the Google form once again.
VIP skill network is C. Yeah, both are same no hurry Prasad just uh learn the fundamentals how fun foundation model works.
Um what is LLM? What is how transformer works? Just go through on that. Okay. I hope all the queries has been sorted. Okay then. Thank you all. Bye.
See regarding the issues please please fill the Google form. See without Google form how do I escalate to you to the team. So that is the reason Google forms has been shared. Please fill the Google form. All your queries will be escalated. I have shared the Google form. Yes. You can after some time as well.
All right. Thank you then. Bye.
